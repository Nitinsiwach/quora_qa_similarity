{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import argparse\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "from spacy.lang.en import English\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import shuffle\n",
    "\n",
    "_PAD = b\"<pad>\"\n",
    "_SOS = b\"<sos>\"\n",
    "_UNK = b\"<unk>\"\n",
    "_START_VOCAB = [_PAD, _SOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "SOS_ID = 1\n",
    "UNK_ID = 2\n",
    "\n",
    "def setup_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    code_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)))\n",
    "    vocab_dir = os.path.join(\"data\", \"quora\")\n",
    "    glove_dir = os.path.join(\"download\", \"dwr\")\n",
    "    data_dir = os.path.join(\"download\", \"quora\")\n",
    "    source_dir = os.path.join(\"data\", \"quora\")\n",
    "    parser.add_argument(\"--source_dir\", default=source_dir)\n",
    "    parser.add_argument(\"--data_dir\", default=data_dir)\n",
    "    parser.add_argument(\"--glove_dir\", default=glove_dir)\n",
    "    parser.add_argument(\"--vocab_dir\", default=vocab_dir)\n",
    "    parser.add_argument(\"--glove_dim\", default=100, type=int)\n",
    "    parser.add_argument(\"--random_init\", default=True, type=bool)\n",
    "    parser.add_argument(\"--split\", default = 0.8, type = float)\n",
    "    parser.add_argument(\"--tokenizer\", default = 'spaCy')\n",
    "    parser.add_argument(\"--question_lower\", default = 5) #this is to discard len(q1) + len(q2) < threshold\n",
    "    parser.add_argument(\"--question_upper\", default = 70) #this is to discard len(q1) or len(q2) > threshold\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def create_txt(data_path, train_path, val_path, split, question_lower, question_upper):\n",
    "    xin = pd.read_csv(data_path)\n",
    "    xin_notnull = xin[~(xin.question1.isnull() | xin.question2.isnull())]\n",
    "    xin_notnull = xin_notnull.sample(frac = 1).reset_index(drop = True)\n",
    "    splitid = int(xin_notnull.shape[0]*split)\n",
    "    xin_train = xin_notnull[:splitid]\n",
    "    xin_val = xin_notnull[splitid:]\n",
    "\n",
    "    question1_train_list = (xin_train.question1.str.replace(\"\\n\", \"\") + \"\\n\").tolist()\n",
    "    question2_train_list = (xin_train.question2.str.replace(\"\\n\", \"\") + \"\\n\").tolist()\n",
    "    question1_val_list = (xin_val.question1.str.replace(\"\\n\", \"\") + \"\\n\").tolist()\n",
    "    question2_val_list = (xin_val.question2.str.replace(\"\\n\", \"\") + \"\\n\").tolist()\n",
    "\n",
    "    train_upper_flag = np.array([False if (len(i.split()) > question_upper) or (len(j.split()) > question_upper) else True for i, j in zip(question1_train_list, question2_train_list)])\n",
    "\n",
    "    val_upper_flag = np.array([False if (len(i.split()) > question_upper) or (len(j.split()) > question_upper) else True for i, j in zip(question1_val_list, question2_val_list)])\n",
    "    question1_train = pd.Series(question1_train_list)[train_upper_flag].tolist()\n",
    "    question2_train = pd.Series(question2_train_list)[train_upper_flag].tolist()\n",
    "    question1_val = pd.Series(question2_val_list)[val_upper_flag].tolist()\n",
    "    question2_val = pd.Series(question2_val_list)[val_upper_flag].tolist()\n",
    "\n",
    "    question1_train = \"\".join(question1_train)\n",
    "    question2_train = \"\".join(question2_train)\n",
    "    question1_val = \"\".join(question1_val)\n",
    "    question2_val = \"\".join(question2_val)\n",
    "    labels_train = \"\".join(map(str, xin_train.is_duplicate[train_upper_flag].tolist()))\n",
    "    labels_val = \"\".join(map(str, xin_val.is_duplicate[val_upper_flag].tolist()))\n",
    "\n",
    "    \n",
    "    with open(train_path + \"\\question1_train.txt\", 'w', encoding = 'utf-8') as q1_t, \\\n",
    "        open(train_path + \"\\question2_train.txt\", 'w', encoding = 'utf-8') as q2_t, \\\n",
    "        open(val_path + \"\\question1_val.txt\", 'w', encoding = 'utf-8') as q1_v, \\\n",
    "        open(val_path + \"\\question2_val.txt\", 'w', encoding = 'utf-8') as q2_v, \\\n",
    "        open(train_path + \"\\labels_train.txt\", 'w', encoding = 'utf-8') as l_t, \\\n",
    "        open(val_path + \"\\labels_val.txt\", 'w', encoding = 'utf-8') as l_v:\n",
    "            q1_t.write(question1_train), \\\n",
    "            q2_t.write(question2_train), \\\n",
    "            q1_v.write(question1_val), \\\n",
    "            q2_v.write(question2_val), \\\n",
    "            l_t.write(labels_train), \\\n",
    "            l_v.write(labels_val)\n",
    "\n",
    "\n",
    "def initialize_vocabulary(vocabulary_path):\n",
    "    # map vocab to word embeddings\n",
    "    '''arguments:\n",
    "            vocabulary_path: vocabulary file with a token in each line\n",
    "        retuns:\n",
    "            vocab, rev_vocab\n",
    "            vocab: a dictionary with signature {'vocab': idx}\n",
    "            rev_vocab: a list of all the tokens in vocabulary_path\n",
    "            There is 1 to 1 mapping between rev_vocab and vocab'''\n",
    "    print('initializing vocabulary')\n",
    "    if gfile.Exists(vocabulary_path):\n",
    "        rev_vocab = []\n",
    "        with gfile.GFile(vocabulary_path, mode=\"r\") as f:\n",
    "            rev_vocab.extend(f.readlines())\n",
    "        rev_vocab = [line.strip('\\n') for line in rev_vocab]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n",
    "\n",
    "\n",
    "def process_glove(args, vocab_list, save_path, size=4e5, random_init=True):\n",
    "    \"\"\"\n",
    "    signature: \n",
    "    creates a numpy matrix glove with word vectors corresonding to tokens in vocab_list\n",
    "    word vec for vocab_list[i] = glove[i]\n",
    "    writes glove to save_path.npz\n",
    "    \n",
    "    :param vocab_list: [vocab]. a list of vocab\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('procesing glove')\n",
    "    if not gfile.Exists(save_path + \".npz\"):\n",
    "        glove_path = os.path.join(args.glove_dir, \"glove.6B.{}d.txt\".format(args.glove_dim))\n",
    "        if random_init:\n",
    "            glove = np.random.randn(len(vocab_list), args.glove_dim)\n",
    "        else:\n",
    "            glove = np.zeros((len(vocab_list), args.glove_dim))\n",
    "        found = 0\n",
    "        with open(glove_path, 'r', encoding = 'utf-8') as fh:\n",
    "            for line in tqdm(fh, total=size):\n",
    "                array = line.lstrip().rstrip().split(\" \")\n",
    "                word = array[0]\n",
    "                vector = list(map(float, array[1:]))\n",
    "                if word in vocab_list:\n",
    "                    idx = vocab_list.index(word)\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "                if word.capitalize() in vocab_list:\n",
    "                    idx = vocab_list.index(word.capitalize())\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "                if word.upper() in vocab_list:\n",
    "                    idx = vocab_list.index(word.upper())\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "\n",
    "        print((\"{}/{} of word vocab have corresponding vectors in {}\".format(found, len(vocab_list), glove_path)))\n",
    "        np.savez_compressed(save_path, glove=glove)\n",
    "        print((\"saved trimmed glove matrix at: {}\".format(save_path)))\n",
    "\n",
    "\n",
    "def create_vocabulary(vocabulary_path, data_paths, tokenizer=None):\n",
    "    '''Iterates through all data_paths and creates a vocab of unique tokens \n",
    "    sorted according to their frequency in collective of data_paths\n",
    "    writes it at vocabulary_path'''\n",
    "    print('creating vocabulary')\n",
    "    if not gfile.Exists(vocabulary_path):\n",
    "        print((\"Creating vocabulary %s from data %s\" % (vocabulary_path, str(data_paths))))\n",
    "        vocab = {}\n",
    "        for path in data_paths:\n",
    "            with open(path, mode=\"rb\") as f:\n",
    "                counter = 0\n",
    "                for line in f:\n",
    "                    counter += 1\n",
    "                    if counter % 100000 == 0:\n",
    "                        print((\"processing line %d\" % counter))\n",
    "                    tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n",
    "                    for w in tokens:\n",
    "                        if w in vocab:\n",
    "                            vocab[w] += 1\n",
    "                        else:\n",
    "                            vocab[w] = 1\n",
    "        vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "        print((\"Vocabulary size: %d\" % len(vocab_list)))\n",
    "        with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n",
    "            for w in vocab_list:\n",
    "                vocab_file.write(w + b\"\\n\")\n",
    "\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocabulary, tokenizer=None):\n",
    "    '''converts sentence to a list  of their token ids according to vocabulary provided\n",
    "    in case a token is not present it is replaced by token id for unk symbol'''\n",
    "    if tokenizer:\n",
    "        words = tokenizer(sentence)\n",
    "        words = [word.orth_ for word in words if not word.orth_.isspace()]\n",
    "    else:\n",
    "        words = basic_tokenizer(sentence)\n",
    "    return [vocabulary.get(w, UNK_ID) for w in words]\n",
    "\n",
    "\n",
    "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
    "                      tokenizer=None):\n",
    "    '''converts file at data_path to a list of token_ids mapped 1 to 1 according to open(vocabulary_path)'''\n",
    "    print('converting data to token ids')\n",
    "    if not gfile.Exists(target_path):\n",
    "        print((\"Tokenizing data in %s\" % data_path))\n",
    "        vocab, _ = initialize_vocabulary(vocabulary_path)\n",
    "        with gfile.GFile(data_path, mode=\"r\") as data_file:\n",
    "            with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
    "                counter = 0\n",
    "                for line in data_file:\n",
    "                    counter += 1\n",
    "                    if counter % 5000 == 0:\n",
    "                        print((\"tokenizing line %d\" % counter))\n",
    "                    token_ids = sentence_to_token_ids(line, vocab, tokenizer)\n",
    "                    tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n",
    "                    \n",
    "def get_tokenizer(name):\n",
    "    if name == 'spaCy':\n",
    "        tokenizer = English()\n",
    "    if name == 'word_tokenize':\n",
    "        tokenizer = word_tokenize\n",
    "    return tokenizer\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = setup_args()\n",
    "    tokenizer = get_tokenizer(args.tokenizer)\n",
    "    vocab_path = pjoin(args.vocab_dir, \"vocab.dat\")\n",
    "    data_path = pjoin(args.data_dir, \"train.csv\")\n",
    "    train_path = pjoin(args.source_dir)\n",
    "    valid_path = pjoin(args.source_dir)\n",
    "    \n",
    "    split = args.split\n",
    "    \n",
    "    create_txt(data_path, train_path, valid_path, split,args.question_lower, args.question_upper)\n",
    "    \n",
    "    create_vocabulary(vocab_path,\n",
    "                      [pjoin(args.source_dir, \"question1_train.txt\"),\n",
    "                       pjoin(args.source_dir, \"question2_train.txt\"),\n",
    "                      pjoin(args.source_dir, \"question1_val.txt\"),\n",
    "                      pjoin(args.source_dir, \"question2_val.txt\")])\n",
    "    vocab, rev_vocab = initialize_vocabulary(vocab_path)\n",
    "\n",
    "    process_glove(args, rev_vocab, args.source_dir + \"/glove.trimmed.{}\".format(args.glove_dim),\n",
    "                  random_init=args.random_init)\n",
    "\n",
    "    question1_train_ids_path = train_path + \"/.ids.train.question1\"\n",
    "    question2_train_ids_path = train_path + \"/.ids.train.question2\"\n",
    "    data_to_token_ids(train_path + \"/question1_train.txt\", question1_train_ids_path, vocab_path, tokenizer = tokenizer)\n",
    "    data_to_token_ids(train_path + \"/question2_train.txt\", question2_train_ids_path, vocab_path, tokenizer = tokenizer)\n",
    "\n",
    "    question1_val_ids_path = valid_path + \"/.ids.val.question1\"\n",
    "    question2_val_ids_path = valid_path + \"/.ids.val.question2\"\n",
    "    data_to_token_ids(valid_path + \"/question1_val.txt\", question1_val_ids_path, vocab_path, tokenizer = tokenizer)\n",
    "    data_to_token_ids(valid_path + \"/question2_val.txt\", question2_val_ids_path, vocab_path, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../code/utils/general_utils.py\"\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Iterates through the provided data one minibatch at at time. You can use this function to\n",
    "    iterate through data in minibatches as follows:\n",
    "\n",
    "        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Or with multiple data sources:\n",
    "\n",
    "        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n",
    "            ...\n",
    "\n",
    "    Args:\n",
    "        data: there are two possible values:\n",
    "            - a list or numpy array\n",
    "            - a list where each element is either a list or numpy array\n",
    "        minibatch_size: the maximum number of items in a minibatch\n",
    "        shuffle: whether to randomize the order of returned data\n",
    "    Returns:\n",
    "        minibatches: the return value depends on data:\n",
    "            - If data is a list/array it yields the next minibatch of data.\n",
    "            - If data a list of lists/arrays it returns the next minibatch of each element in the\n",
    "              list. This can be used to iterate through multiple data sources\n",
    "              (e.g., features and labels) at the same time.\n",
    "\n",
    "    \"\"\"\n",
    "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
    "    data_size = len(data[0]) if list_data else len(data)\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
    "            else minibatch(data, minibatch_indices)\n",
    "\n",
    "\n",
    "def minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "\n",
    "def test_all_close(name, actual, expected):\n",
    "    if actual.shape != expected.shape:\n",
    "        raise ValueError(\"{:} failed, expected output to have shape {:} but has shape {:}\"\n",
    "                         .format(name, expected.shape, actual.shape))\n",
    "    if np.amax(np.fabs(actual - expected)) > 1e-6:\n",
    "        raise ValueError(\"{:} failed, expected {:} but value is {:}\".format(name, expected, actual))\n",
    "    else:\n",
    "        print((name, \"passed!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abcd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-316-5a8c7f3809f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mabcd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'abcd' is not defined"
     ]
    }
   ],
   "source": [
    "abcd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1,2,3,4]) == np.array([2,2,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"model.py\"\n",
    "\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from utils.general_utils import get_minibatches\n",
    "import os\n",
    "import pickle\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "_BIAS_VARIABLE_NAME = \"bias\"\n",
    "_WEIGHTS_VARIABLE_NAME = \"kernel\"\n",
    "def get_optimizer(opt):\n",
    "    if opt == \"adam\":\n",
    "        optfn = tf.train.AdamOptimizer\n",
    "    elif opt == \"sgd\":\n",
    "        optfn = tf.train.GradientDescentOptimizer\n",
    "    else:\n",
    "        assert (False)\n",
    "    return optfn\n",
    " \n",
    "        \n",
    "class Encoder():\n",
    "    def __init__(self, encoder_size):\n",
    "        print(\"building encoder\")\n",
    "        self.size = encoder_size\n",
    "\n",
    "    def encode(self, inputs, masks):\n",
    "        \"\"\"\n",
    "        In a generalized encode function, you pass in your inputs,\n",
    "        masks, and an initial\n",
    "        hidden state input into this function.\n",
    "\n",
    "        :param inputs: Symbolic representations of your input\n",
    "         masks: this is to make sure tf.nn.dynamic_rnn doesn't iterate\n",
    "                      through masked steps\n",
    "        :return: an encoded representation of your input.\n",
    "                 It can be context-level representation, word-level representation,\n",
    "                 or both.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"encoder\") as scope_encoder:\n",
    "            #compute sequence length\n",
    "            sequence_lengths = tf.reduce_sum(masks, axis = 1) \n",
    "            #create a forward cell\n",
    "            fw_cell = tf.contrib.rnn.LSTMCell(self.size)\n",
    "\n",
    "            #pass the cells to bilstm and create the bilstm\n",
    "            bw_cell = tf.contrib.rnn.LSTMCell(self.size)\n",
    "            output, final_state = tf.nn.bidirectional_dynamic_rnn(fw_cell, \\\n",
    "                                                                  bw_cell, inputs, \\\n",
    "                                                                  sequence_length = sequence_lengths, \\\n",
    "                                                                  dtype = tf.float32, \\\n",
    "                                                                  parallel_iterations = 256)\n",
    "            output_lstm = tf.concat([output[0], output[1]], axis = -1)\n",
    "            final_state_lstm = tf.concat([final_state[0], final_state[1]], axis = -1)\n",
    "            print(output_lstm)\n",
    "            return output_lstm, final_state_lstm\n",
    "    \n",
    "    \n",
    "\n",
    "class QSystem(object):\n",
    "    def __init__(self, encoder, pretrained_embeddings, config, train_flag = True):\n",
    "        \"\"\"\n",
    "        Initializes your System\n",
    "\n",
    "        :param encoder: an encoder that you constructed in train.py\n",
    "        :param decoder: a decoder that you constructed in train.py\n",
    "        :param args: pass in more arguments as needed\n",
    "        \"\"\"\n",
    "\n",
    "        # ==== set up placeholder tokens ========\n",
    "        print(\"building question similarity calculator\")\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.pretrained_embeddings_vars = tf.constant(pretrained_embeddings, dtype = tf.float32)\n",
    "        \n",
    "        self.embed_dim = config['embed_dim']\n",
    "        self.optimizer = config['optimizer']\n",
    "        self.minibatch_size = config['minibatch_size']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.max_grad_norm = config['max_grad_norm']\n",
    "        self.max_sent_len = config['max_sent_len']\n",
    "        self.encoder_size = config['encoder_size']\n",
    "        \n",
    "        self.q1_masks = tf.placeholder(tf.int32, shape = [None, self.max_sent_len])\n",
    "        self.q2_masks = tf.placeholder(tf.int32, shape = [None, self.max_sent_len])\n",
    "        \n",
    "        self.q1_id = tf.placeholder(tf.int32, shape = [None, self.max_sent_len]) #batch_size x question_length\n",
    "        self.q2_id = tf.placeholder(tf.int32, shape = [None, self.max_sent_len]) #batch_size x sent_max_length\n",
    "\n",
    "        self.a = tf.placeholder(tf.int32, shape = [None, ]) #batch_size x 1\n",
    "        # ==== assemble pieces ====\n",
    "        with tf.variable_scope(\"qa\"):\n",
    "            self.embed_lookup()\n",
    "            self.setup_system()\n",
    "            self.setup_loss()\n",
    "            self.make_optimizer()\n",
    "            self.saver = self.saver_prot()\n",
    "\n",
    "        # ==== set up training/updating procedure ====\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def setup_system(self):\n",
    "        \"\"\"\n",
    "        After your modularized implementation of encoder and decoder\n",
    "        you should call various functions inside encoder, decoder here\n",
    "        to assemble your reading comprehension system!\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"questionRNN\", reuse=tf.AUTO_REUSE):\n",
    "            encoder_q1, final_state_q1 = self.encoder.encode(self.q1, self.q1_masks)\n",
    "            encoder_q2, final_state_q2 = self.encoder.encode(self.q2, self.q2_masks)\n",
    "        dot = tf.multiply(encoder_q1,encoder_q2)\n",
    "        diff = encoder_q1 - encoder_q2\n",
    "        batch = tf.shape(diff)[0]\n",
    "        similarity = tf.concat([encoder_q1, encoder_q2, dot, diff], axis = -1)\n",
    "        similarity = tf.reshape(similarity, [batch, self.max_sent_len*self.encoder_size*2*4])\n",
    "        with tf.variable_scope(\"affine\", regularizer = tf.contrib.layers.l2_regularizer(0.001)):\n",
    "            self.logits = tf.contrib.layers.fully_connected(similarity, 2, activation_fn=None) #batch, 2\n",
    "            \n",
    "        \n",
    "        self.prediction = tf.argmax(self.logits, axis = 1) #batch\n",
    "\n",
    "    def setup_loss(self):\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = self.a, logits = self.logits))\n",
    "#             + tf.losses.get_regularization_loss()\n",
    "            \n",
    "            \n",
    "            \n",
    "    def make_optimizer(self):\n",
    "        optimizer = get_optimizer(self.optimizer)\n",
    "        _optimizer_op = optimizer(self.learning_rate)\n",
    "        gradients, variables = zip(*_optimizer_op.compute_gradients(self.loss))\n",
    "        clipped_gradients, self.global_norm = tf.clip_by_global_norm(gradients, self.max_grad_norm)\n",
    "        self.optimizer_op = _optimizer_op.apply_gradients(zip(gradients, variables))\n",
    "        self.optimizer_op = _optimizer_op.minimize(self.loss)\n",
    "    \n",
    "\n",
    "    def embed_lookup(self):\n",
    "        self.q1 = tf.nn.embedding_lookup(self.pretrained_embeddings_vars, self.q1_id) #batch,q_max_len,embed_dim\n",
    "        self.q2 = tf.nn.embedding_lookup(self.pretrained_embeddings_vars, self.q2_id)\n",
    "\n",
    "    def validate(self, session, valid_dataset):\n",
    "        \"\"\"\n",
    "        Iterate through the validation dataset and determine what\n",
    "        the validation cost is.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        valid_cost = 0\n",
    "\n",
    "        input_feed = self.feed_dict(valid_dataset)\n",
    "        output_feed = [self.loss]\n",
    "        valid_cost = session.run(output_feed, input_feed)\n",
    "        \n",
    "        return valid_cost\n",
    "    \n",
    "    def predict(self, session, prediction_dataset):\n",
    "        input_feed = self.feed_dict(prediction_dataset)\n",
    "        output = self.prediction\n",
    "        \n",
    "        predictions = session.run(output, input_feed)\n",
    "        return predictions\n",
    "        \n",
    "\n",
    "    def evaluate_answer(self, session, dataset, log=False):\n",
    "        \"\"\"\n",
    "        dataset: [q1, q2, a]\n",
    "        naive accuracy measure\n",
    "        \"\"\"\n",
    "        predictions = np.array(self.predict(session, dataset))\n",
    "        \n",
    "        gold = np.array(dataset[2])\n",
    "        print(predictions.shape,gold.shape)\n",
    "#         print(predictions[:10] == gold[:10] )\n",
    "        correct = predictions == gold\n",
    "        print(correct)\n",
    "        accuracy = np.sum(correct)/gold.shape\n",
    "        \n",
    "        \n",
    "        if log:\n",
    "            logging.info(\"accuracy: {}\".format(accuracy))\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def pad(self, datalist):\n",
    "        '''pads q1 and q2 to max length of q1 and q2\n",
    "        Params: datalist: [q1,q2]\n",
    "                where q1, q2: [[w1, w2, ..], [w6,w7, ..]]'''\n",
    "        \n",
    "\n",
    "        padded = []\n",
    "        masks = []\n",
    "        m_len = self.max_sent_len\n",
    "        q1 = datalist[0]\n",
    "        q2 = datalist[1]\n",
    "        q1 = [i[:70] if len(i) > m_len else i for i in q1]\n",
    "        q2 = [i[:70] if len(i) > m_len else i for i in q2]\n",
    "        datalist = [q1, q2]\n",
    "        padded = [[k + [0]*(m_len-len(k)) for k in j] for j in datalist]\n",
    "        masks = [[[1 if t != 0 else 0 for t in k] for k in j] for j in padded]\n",
    "        return padded, masks\n",
    "    \n",
    "    def feed_dict(self, dataset_feed, mode = 'train'):\n",
    "        '''dataset_feed: ([q1,q2,a])'''\n",
    "        input_feed = {}\n",
    "        if mode == 'train':\n",
    "            a = dataset_feed[2]\n",
    "            input_feed[self.a] = a\n",
    "        \n",
    "        padded, padded_masks = self.pad(dataset_feed[:2])\n",
    "        input_feed[self.q1_id], input_feed[self.q2_id] = padded\n",
    "        input_feed[self.q1_masks], input_feed[self.q2_masks]= padded_masks\n",
    "        return input_feed\n",
    "         \n",
    "    def run_epoch(self, dataset, sess):\n",
    "        '''dataset is a list [q1, q2, a]'''\n",
    "        \n",
    "        n_minibatches = 0.\n",
    "        total_loss = 0.\n",
    "        for dataset_mini in get_minibatches(dataset, self.minibatch_size):\n",
    "            n_minibatches += 1\n",
    "            feed_dict = self.feed_dict(dataset_mini)\n",
    "            output = [self.optimizer_op , self.loss, self.global_norm]\n",
    "            _, loss, global_norm = sess.run(output, feed_dict)\n",
    "            if not n_minibatches % 1:\n",
    "                print(\"n_minibatch = {}\".format(n_minibatches), \"loss: {}\".format(loss), \"global_norm{}\".format(global_norm))\n",
    "            total_loss += loss  \n",
    "        return total_loss/n_minibatches\n",
    "    \n",
    "    def saver_prot(self):\n",
    "        return tf.train.Saver()\n",
    "\n",
    "    def train(self, session, dataset,sent_max_len, epochs, train_dir, test):\n",
    "        \"\"\"\n",
    "        Implement main training loop\n",
    "        :param session: it should be passed in from train.py\n",
    "               dataset: a representation of our data, in some implementations, you can\n",
    "                        pass in multiple components (arguments) of one dataset to this function.\n",
    "                        In this implimentation it is passed down as a list of train and val:\n",
    "                        [[train_q1, train_q2, train_a], [val_q1, val_q2, val_a]]\n",
    "                        \n",
    "               train_dir: path to the directory where you save the model checkpoint\n",
    "        :return: best_score\n",
    "        \"\"\"                                                  \n",
    "        results_path = os.path.join(train_dir, \"{:%Y%m%d_%H%M%S}\".format(datetime.now()))\n",
    "        tic = time.time()\n",
    "        params = tf.trainable_variables()\n",
    "        num_params = sum(map(lambda t: np.prod(tf.shape(t.value()).eval()), params))\n",
    "        toc = time.time()\n",
    "        logging.info(\"Number of params: %d (retreival took %f secs)\" % (num_params, toc - tic))\n",
    "        \n",
    "       \n",
    "        dataset_train = dataset[0]\n",
    "        dataset_val = dataset[1]\n",
    "        best_score = 0\n",
    "        accuracy = -1\n",
    "#         best_score, _ = self.evaluate_answer(session, dataset_val,log=True)\n",
    "    \n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            logging.info(\"Epoch %d out of %d\", epoch + 1, epochs)\n",
    "            logging.info(\"Best score so far: \" + str(best_score))\n",
    "            loss = self.run_epoch(dataset_train, session)\n",
    "            print(len(dataset_train[0]),len(dataset_train[1]),len(dataset_train[2]))\n",
    "            accuracy = self.evaluate_answer(session, dataset_train, log=True)\n",
    "            logging.info(\"loss: \" + str(loss) + \" accuracy: \"+str(accuracy))\n",
    "            if accuracy > best_score:\n",
    "                best_score = accuracy\n",
    "                logging.info(\"New best score! Saving model in %s\", results_path)\n",
    "                self.saver.save(session, results_path)    \n",
    "            print(\"\")\n",
    "\n",
    "        return best_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from model import Encoder, QSystem\n",
    "from os.path import join as pjoin\n",
    "import argparse\n",
    "\n",
    "import logging\n",
    "import data.py\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--test\", default = False, type = bool)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "class _FLAGS():\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.max_gradient_norm = 5.0\n",
    "        self.dropout = 0.15\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 60\n",
    "        self.state_size = 10\n",
    "        self.embedding_size = 300\n",
    "        self.data_dir = \"data/quora\"\n",
    "        self.train_dir = \"train\"\n",
    "        self.load_train_dir = \"\"\n",
    "        self.log_dir = \"log\"\n",
    "        self.optimizer = \"adam\"\n",
    "        self.vocab_path = \"data/quora/vocab.dat\"\n",
    "        self.max_sent_len = 70\n",
    "FLAGS = _FLAGS()\n",
    "\n",
    "def initialize_model(session, model, train_dir):\n",
    "    ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "    v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "    if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "        logging.info(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        logging.info(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        logging.info('Num params: %d' % sum(v.get_shape().num_elements() for v in tf.trainable_variables()))\n",
    "    return model\n",
    "\n",
    "def get_device_name():\n",
    "    print('device in use:',tf.test.gpu_device_name() )\n",
    "    return 'CPU' if tf.test.gpu_device_name() == '' else tf.test.gpu_device_name()\n",
    "\n",
    "def get_normalized_train_dir(train_dir):\n",
    "    \"\"\"\n",
    "    Adds symlink to {train_dir} from /tmp/cs224n-squad-train to canonicalize the\n",
    "    file paths saved in the checkpoint. This allows the model to be reloaded even\n",
    "    if the location of the checkpoint files has moved, allowing usage with CodaLab.\n",
    "    This must be done on both train.py and qa_answer.py in order to work.\n",
    "    \"\"\"\n",
    "    global_train_dir = 'dir_node_wordvar'\n",
    "    if os.path.exists(global_train_dir):\n",
    "#         os.unlink(global_train_dir) #forlinux\n",
    "        os.system('rmdir \"%s\"' % global_train_dir)\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    os.symlink(os.path.abspath(train_dir), global_train_dir, True)\n",
    "    return global_train_dir\n",
    "\n",
    "def get_pretrained_embeddings(embed_path):\n",
    "    glove = np.load(embed_path)\n",
    "    return glove['glove']\n",
    "\n",
    "\n",
    "def main():\n",
    "    get_device_name()\n",
    "\n",
    "    print('Device in use {}'.format(get_device_name()))\n",
    "    dataset = None\n",
    "\n",
    "    embed_path = pjoin(\"data\", \"quora\", \"glove.trimmed.{}.npz\".format(FLAGS.embedding_size))\n",
    "    vocab_path = FLAGS.vocab_path or pjoin(FLAGS.data_dir, \"vocab.dat\")\n",
    "    pretrained_embeddings = get_pretrained_embeddings(embed_path)\n",
    "    config = {}\n",
    "    config['embed_dim'] = FLAGS.embedding_size\n",
    "    config['optimizer'] = FLAGS.optimizer\n",
    "    config['minibatch_size'] = FLAGS.batch_size\n",
    "    config['learning_rate'] = FLAGS.learning_rate\n",
    "    config['max_grad_norm'] = FLAGS.max_gradient_norm\n",
    "    config['max_sent_len'] = FLAGS.max_sent_len\n",
    "    config['encoder_size'] = FLAGS.state_size\n",
    "\n",
    "    encoder = Encoder(FLAGS.state_size)\n",
    "    qsystem = QSystem(encoder, pretrained_embeddings, config)\n",
    "\n",
    "    if not os.path.exists(FLAGS.log_dir):\n",
    "        os.makedirs(FLAGS.log_dir)\n",
    "    file_handler = logging.FileHandler(pjoin(FLAGS.log_dir, \"log.txt\"))\n",
    "    logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "    with open(vocab_path, encoding = 'utf-8') as file1:\n",
    "        f = file1.readlines()\n",
    "\n",
    "    print((vars(FLAGS)))\n",
    "\n",
    "    with open(\"data/quora\"+\"/.ids.train.question1\", encoding = 'utf8') as tq1, \\\n",
    "        open(\"data/quora\"+\"/.ids.train.question2\", encoding = 'utf8') as tq2, \\\n",
    "        open(\"data/quora\"+\"/.ids.val.question1\", encoding = 'utf8') as vq1, \\\n",
    "        open(\"data/quora\"+\"/.ids.val.question2\", encoding = 'utf8') as vq2, \\\n",
    "        open(\"data/quora\"+\"/labels_train.txt\", encoding = 'utf8') as ta, \\\n",
    "        open(\"data/quora\"+\"/labels_val.txt\", encoding = 'utf8') as va:\n",
    "                tq1 = tq1.readlines()\n",
    "                tq2 = tq2.readlines()\n",
    "                ta = ta.readlines()\n",
    "                vq1 = vq1.readlines()\n",
    "                vq2 = vq2.readlines()\n",
    "                va = va.readlines()\n",
    "    ta = [int(i) for i in list(ta[0])]\n",
    "    va = [int(i) for i in list(va[0])]\n",
    "    tq1 = [[int(i) for i in j.replace(\"\\n\", \"\").split()] for j in tq1]\n",
    "    tq2 = [[int(i) for i in j.replace(\"\\n\", \"\").split()] for j in tq2]\n",
    "    vq1 = [[int(i) for i in j.replace(\"\\n\", \"\").split()] for j in vq1]\n",
    "    vq2 = [[int(i) for i in j.replace(\"\\n\", \"\").split()] for j in vq2]\n",
    "    dataset = [[tq1[1560:51560],tq2[1560:51560],ta[1560:51560]],[vq1[1560:51560],vq2[1560:51560],va[1560:51560]]]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        load_train_dir = get_normalized_train_dir(FLAGS.load_train_dir or FLAGS.train_dir)\n",
    "        initialize_model(sess, qsystem, load_train_dir)\n",
    "\n",
    "        save_train_dir = get_normalized_train_dir(FLAGS.train_dir)\n",
    "        qsystem.train(sess, dataset, FLAGS.max_sent_len, FLAGS.epochs, save_train_dir, test = args.test)\n",
    "#         qa.evaluate_answer(sess, dataset_val, FLAGS.evaluation_size, log=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import argparse\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "from spacy.lang.en import English\n",
    "from nltk.tokenize import word_tokenize\n",
    "from random import shuffle\n",
    "\n",
    "_PAD = b\"<pad>\"\n",
    "_SOS = b\"<sos>\"\n",
    "_UNK = b\"<unk>\"\n",
    "_START_VOCAB = [_PAD, _SOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "SOS_ID = 1\n",
    "UNK_ID = 2\n",
    "\n",
    "def setup_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    code_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)))\n",
    "    vocab_dir = os.path.join(\"data\", \"quora\")\n",
    "    glove_dir = os.path.join(\"download\", \"dwr\")\n",
    "    data_dir = os.path.join(\"download\", \"quora\")\n",
    "    source_dir = os.path.join(\"data\", \"quora\")\n",
    "    parser.add_argument(\"--source_dir\", default=source_dir)\n",
    "    parser.add_argument(\"--data_dir\", default=data_dir)\n",
    "    parser.add_argument(\"--glove_dir\", default=glove_dir)\n",
    "    parser.add_argument(\"--vocab_dir\", default=vocab_dir)\n",
    "    parser.add_argument(\"--glove_dim\", default=100, type=int)\n",
    "    parser.add_argument(\"--random_init\", default=True, type=bool)\n",
    "    parser.add_argument(\"--split\", default = 0.8, type = float)\n",
    "    parser.add_argument(\"--tokenizer\", default = 'spaCy')\n",
    "    parser.add_argument(\"--question_lower\", default = 5) #this is to discard len(q1) + len(q2) < threshold\n",
    "    parser.add_argument(\"--question_upper\", default = 70) #this is to discard len(q1) or len(q2) > threshold\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def create_txt(data_path, train_path, val_path, split, question_lower, question_upper):\n",
    "    xin = pd.read_csv(data_path)\n",
    "    xin_notnull = xin[~(xin.question1.isnull() | xin.question2.isnull())]\n",
    "    xin_notnull = xin_notnull.sample(frac = 1).reset_index(drop = True)\n",
    "    splitid = int(xin_notnull.shape[0]*split)\n",
    "    xin_train = xin_notnull[:splitid]\n",
    "    xin_val = xin_notnull[splitid:]\n",
    "\n",
    "    question1_train_list = (xin_train.question1.str.replace(\"\\n\", \"\") + \"\\n\").tolist()\n",
    "    question2_train_list = (xin_train.question2.str.replace(\"\\n\", \"\") + \"\\n\").tolist()\n",
    "    question1_val_list = (xin_val.question1.str.replace(\"\\n\", \"\") + \"\\n\").tolist()\n",
    "    question2_val_list = (xin_val.question2.str.replace(\"\\n\", \"\") + \"\\n\").tolist()\n",
    "\n",
    "    train_upper_flag = np.array([False if (len(i.split()) > question_upper) or (len(j.split()) > question_upper) else True for i, j in zip(question1_train_list, question2_train_list)])\n",
    "\n",
    "    val_upper_flag = np.array([False if (len(i.split()) > question_upper) or (len(j.split()) > question_upper) else True for i, j in zip(question1_val_list, question2_val_list)])\n",
    "    question1_train = pd.Series(question1_train_list)[train_upper_flag].tolist()\n",
    "    question2_train = pd.Series(question2_train_list)[train_upper_flag].tolist()\n",
    "    question1_val = pd.Series(question2_val_list)[val_upper_flag].tolist()\n",
    "    question2_val = pd.Series(question2_val_list)[val_upper_flag].tolist()\n",
    "\n",
    "    question1_train = \"\".join(question1_train)\n",
    "    question2_train = \"\".join(question2_train)\n",
    "    question1_val = \"\".join(question1_val)\n",
    "    question2_val = \"\".join(question2_val)\n",
    "    labels_train = \"\".join(map(str, xin_train.is_duplicate[train_upper_flag].tolist()))\n",
    "    labels_val = \"\".join(map(str, xin_val.is_duplicate[val_upper_flag].tolist()))\n",
    "\n",
    "    \n",
    "    with open(train_path + \"\\question1_train.txt\", 'w', encoding = 'utf-8') as q1_t, \\\n",
    "        open(train_path + \"\\question2_train.txt\", 'w', encoding = 'utf-8') as q2_t, \\\n",
    "        open(val_path + \"\\question1_val.txt\", 'w', encoding = 'utf-8') as q1_v, \\\n",
    "        open(val_path + \"\\question2_val.txt\", 'w', encoding = 'utf-8') as q2_v, \\\n",
    "        open(train_path + \"\\labels_train.txt\", 'w', encoding = 'utf-8') as l_t, \\\n",
    "        open(val_path + \"\\labels_val.txt\", 'w', encoding = 'utf-8') as l_v:\n",
    "            q1_t.write(question1_train), \\\n",
    "            q2_t.write(question2_train), \\\n",
    "            q1_v.write(question1_val), \\\n",
    "            q2_v.write(question2_val), \\\n",
    "            l_t.write(labels_train), \\\n",
    "            l_v.write(labels_val)\n",
    "\n",
    "\n",
    "def initialize_vocabulary(vocabulary_path):\n",
    "    # map vocab to word embeddings\n",
    "    '''arguments:\n",
    "            vocabulary_path: vocabulary file with a token in each line\n",
    "        retuns:\n",
    "            vocab, rev_vocab\n",
    "            vocab: a dictionary with signature {'vocab': idx}\n",
    "            rev_vocab: a list of all the tokens in vocabulary_path\n",
    "            There is 1 to 1 mapping between rev_vocab and vocab'''\n",
    "    print('initializing vocabulary')\n",
    "    if gfile.Exists(vocabulary_path):\n",
    "        rev_vocab = []\n",
    "        with gfile.GFile(vocabulary_path, mode=\"r\") as f:\n",
    "            rev_vocab.extend(f.readlines())\n",
    "        rev_vocab = [line.strip('\\n') for line in rev_vocab]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n",
    "\n",
    "\n",
    "def process_glove(args, vocab_list, save_path, size=4e5, random_init=True):\n",
    "    \"\"\"\n",
    "    signature: \n",
    "    creates a numpy matrix glove with word vectors corresonding to tokens in vocab_list\n",
    "    word vec for vocab_list[i] = glove[i]\n",
    "    writes glove to save_path.npz\n",
    "    \n",
    "    :param vocab_list: [vocab]. a list of vocab\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('procesing glove')\n",
    "    if not gfile.Exists(save_path + \".npz\"):\n",
    "        glove_path = os.path.join(args.glove_dir, \"glove.6B.{}d.txt\".format(args.glove_dim))\n",
    "        if random_init:\n",
    "            glove = np.random.randn(len(vocab_list), args.glove_dim)\n",
    "        else:\n",
    "            glove = np.zeros((len(vocab_list), args.glove_dim))\n",
    "        found = 0\n",
    "        with open(glove_path, 'r', encoding = 'utf-8') as fh:\n",
    "            for line in tqdm(fh, total=size):\n",
    "                array = line.lstrip().rstrip().split(\" \")\n",
    "                word = array[0]\n",
    "                vector = list(map(float, array[1:]))\n",
    "                if word in vocab_list:\n",
    "                    idx = vocab_list.index(word)\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "                if word.capitalize() in vocab_list:\n",
    "                    idx = vocab_list.index(word.capitalize())\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "                if word.upper() in vocab_list:\n",
    "                    idx = vocab_list.index(word.upper())\n",
    "                    glove[idx, :] = vector\n",
    "                    found += 1\n",
    "\n",
    "        print((\"{}/{} of word vocab have corresponding vectors in {}\".format(found, len(vocab_list), glove_path)))\n",
    "        np.savez_compressed(save_path, glove=glove)\n",
    "        print((\"saved trimmed glove matrix at: {}\".format(save_path)))\n",
    "\n",
    "\n",
    "def create_vocabulary(vocabulary_path, data_paths, tokenizer=None):\n",
    "    '''Iterates through all data_paths and creates a vocab of unique tokens \n",
    "    sorted according to their frequency in collective of data_paths\n",
    "    writes it at vocabulary_path'''\n",
    "    print('creating vocabulary')\n",
    "    if not gfile.Exists(vocabulary_path):\n",
    "        print((\"Creating vocabulary %s from data %s\" % (vocabulary_path, str(data_paths))))\n",
    "        vocab = {}\n",
    "        for path in data_paths:\n",
    "            with open(path, mode=\"rb\") as f:\n",
    "                counter = 0\n",
    "                for line in f:\n",
    "                    counter += 1\n",
    "                    if counter % 100000 == 0:\n",
    "                        print((\"processing line %d\" % counter))\n",
    "                    tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n",
    "                    for w in tokens:\n",
    "                        if w in vocab:\n",
    "                            vocab[w] += 1\n",
    "                        else:\n",
    "                            vocab[w] = 1\n",
    "        vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "        print((\"Vocabulary size: %d\" % len(vocab_list)))\n",
    "        with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n",
    "            for w in vocab_list:\n",
    "                vocab_file.write(w + b\"\\n\")\n",
    "\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocabulary, tokenizer=None):\n",
    "    '''converts sentence to a list  of their token ids according to vocabulary provided\n",
    "    in case a token is not present it is replaced by token id for unk symbol'''\n",
    "    if tokenizer:\n",
    "        words = tokenizer(sentence)\n",
    "        words = [word.orth_ for word in words if not word.orth_.isspace()]\n",
    "    else:\n",
    "        words = basic_tokenizer(sentence)\n",
    "    return [vocabulary.get(w, UNK_ID) for w in words]\n",
    "\n",
    "\n",
    "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
    "                      tokenizer=None):\n",
    "    '''converts file at data_path to a list of token_ids mapped 1 to 1 according to open(vocabulary_path)'''\n",
    "    print('converting data to token ids')\n",
    "    if not gfile.Exists(target_path):\n",
    "        print((\"Tokenizing data in %s\" % data_path))\n",
    "        vocab, _ = initialize_vocabulary(vocabulary_path)\n",
    "        with gfile.GFile(data_path, mode=\"r\") as data_file:\n",
    "            with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
    "                counter = 0\n",
    "                for line in data_file:\n",
    "                    counter += 1\n",
    "                    if counter % 5000 == 0:\n",
    "                        print((\"tokenizing line %d\" % counter))\n",
    "                    token_ids = sentence_to_token_ids(line, vocab, tokenizer)\n",
    "                    tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n",
    "                    \n",
    "def get_tokenizer(name):\n",
    "    if name == 'spaCy':\n",
    "        tokenizer = English()\n",
    "    if name == 'word_tokenize':\n",
    "        tokenizer = word_tokenize\n",
    "    return tokenizer\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = setup_args()\n",
    "    tokenizer = get_tokenizer(args.tokenizer)\n",
    "    vocab_path = pjoin(args.vocab_dir, \"vocab.dat\")\n",
    "    data_path = pjoin(args.data_dir, \"train.csv\")\n",
    "    train_path = pjoin(args.source_dir)\n",
    "    valid_path = pjoin(args.source_dir)\n",
    "    \n",
    "    split = args.split\n",
    "    \n",
    "    create_txt(data_path, train_path, valid_path, split,args.question_lower, args.question_upper)\n",
    "    \n",
    "    create_vocabulary(vocab_path,\n",
    "                      [pjoin(args.source_dir, \"question1_train.txt\"),\n",
    "                       pjoin(args.source_dir, \"question2_train.txt\"),\n",
    "                      pjoin(args.source_dir, \"question1_val.txt\"),\n",
    "                      pjoin(args.source_dir, \"question2_val.txt\")])\n",
    "    vocab, rev_vocab = initialize_vocabulary(vocab_path)\n",
    "\n",
    "    process_glove(args, rev_vocab, args.source_dir + \"/glove.trimmed.{}\".format(args.glove_dim),\n",
    "                  random_init=args.random_init)\n",
    "\n",
    "    question1_train_ids_path = train_path + \"/.ids.train.question1\"\n",
    "    question2_train_ids_path = train_path + \"/.ids.train.question2\"\n",
    "    data_to_token_ids(train_path + \"/question1_train.txt\", question1_train_ids_path, vocab_path, tokenizer = tokenizer)\n",
    "    data_to_token_ids(train_path + \"/question2_train.txt\", question2_train_ids_path, vocab_path, tokenizer = tokenizer)\n",
    "\n",
    "    question1_val_ids_path = valid_path + \"/.ids.val.question1\"\n",
    "    question2_val_ids_path = valid_path + \"/.ids.val.question2\"\n",
    "    data_to_token_ids(valid_path + \"/question1_val.txt\", question1_val_ids_path, vocab_path, tokenizer = tokenizer)\n",
    "    data_to_token_ids(valid_path + \"/question2_val.txt\", question2_val_ids_path, vocab_path, tokenizer = tokenizer)\n",
    "    \n",
    "    question1_val_ids_path = v_path + \"/.ids.val.question1\"\n",
    "    question2_val_ids_path = valid_path + \"/.ids.val.question2\"\n",
    "    data_to_token_ids(test_path + \"/question1_val.txt\", question1_val_ids_path, vocab_path, tokenizer = tokenizer)\n",
    "    data_to_token_ids(test_path + \"/question2_val.txt\", question2_val_ids_path, vocab_path, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../../nlp/cs224n/assignment4/assignment4/code/traincolab_dmn_wordvar.py\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from qa_model_dmn_wordvar import Encoder, QASystem, Decoder, Config, EpisodicMemoryCell, EpisodicMemory\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class _FLAGS():\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.0003\n",
    "        self.max_gradient_norm = 5.0\n",
    "        self.dropout = 0.15\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 40\n",
    "        self.state_size = 250\n",
    "        self.output_size =  750\n",
    "        self.embedding_size = 100\n",
    "        self.data_dir = \"data/squad\"\n",
    "        self.train_dir = \"traindmn_wordvar\"\n",
    "        self.load_train_dir = \"\"\n",
    "        self.log_dir = \"log\"\n",
    "        self.optimizer = \"adam\"\n",
    "        self.print_every = 1\n",
    "        self.keep = 0\n",
    "        self.vocab_path = \"data/squad/vocab.dat\"\n",
    "        self.embed_path = \"\"\n",
    "        self.evaluation_size = 500\n",
    "        self.n_layers = 5\n",
    "        self.attention_hidden_units = 500\n",
    "FLAGS = _FLAGS()\n",
    "\n",
    "def initialize_model(session, model, train_dir):\n",
    "    ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "    v2_path = ckpt.model_checkpoint_path + \".index\" if ckpt else \"\"\n",
    "    if ckpt and (tf.gfile.Exists(ckpt.model_checkpoint_path) or tf.gfile.Exists(v2_path)):\n",
    "        logging.info(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        logging.info(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        logging.info('Num params: %d' % sum(v.get_shape().num_elements() for v in tf.trainable_variables()))\n",
    "    return model\n",
    "\n",
    "def get_device_name():\n",
    "    print('device in use:',tf.test.gpu_device_name() )\n",
    "    return 'CPU' if tf.test.gpu_device_name() == '' else tf.test.gpu_device_name()\n",
    "\n",
    "def get_normalized_train_dir(train_dir):\n",
    "    \"\"\"\n",
    "    Adds symlink to {train_dir} from /tmp/cs224n-squad-train to canonicalize the\n",
    "    file paths saved in the checkpoint. This allows the model to be reloaded even\n",
    "    if the location of the checkpoint files has moved, allowing usage with CodaLab.\n",
    "    This must be done on both train.py and qa_answer.py in order to work.\n",
    "    \"\"\"\n",
    "    global_train_dir = 'dir_node_wordvar'\n",
    "    if os.path.exists(global_train_dir):\n",
    "        os.unlink(global_train_dir) #forlinux\n",
    "#         os.system('rmdir \"%s\"' % global_train_dir)\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    os.symlink(os.path.abspath(train_dir), global_train_dir, True)\n",
    "    return global_train_dir\n",
    "\n",
    "def get_pretrained_embeddings(embed_path):\n",
    "    glove = np.load(embed_path)\n",
    "    return glove['glove']\n",
    "\n",
    "\n",
    "def main():\n",
    "    get_device_name()\n",
    "    \n",
    "    print('Device in use {}'.format(get_device_name()))\n",
    "\n",
    "    # Do what you need to load datasets from FLAGS.data_dir\n",
    "    dataset = None\n",
    "\n",
    "    embed_path = FLAGS.embed_path or pjoin(\"data\", \"squad\", \"glove.trimmed.{}.npz\".format(FLAGS.embedding_size))\n",
    "    vocab_path = FLAGS.vocab_path or pjoin(FLAGS.data_dir, \"vocab.dat\")\n",
    "    pretrained_embeddings = get_pretrained_embeddings(embed_path)\n",
    "    \n",
    "    config = Config(FLAGS.embedding_size, FLAGS.evaluation_size, FLAGS.optimizer, FLAGS.batch_size, FLAGS.learning_rate, \\\n",
    "                   FLAGS.max_gradient_norm)\n",
    "    cell = EpisodicMemoryCell(2*FLAGS.state_size)\n",
    "    episodicmemory = EpisodicMemory(cell, FLAGS.n_layers, FLAGS.state_size, FLAGS.attention_hidden_units)\n",
    "    encoder = Encoder(FLAGS.state_size, vocab_dim=FLAGS.embedding_size)\n",
    "    decoder = Decoder(decoder_size=FLAGS.output_size)\n",
    "    \n",
    "    qa = QASystem(encoder, decoder, episodicmemory, pretrained_embeddings, config)\n",
    "\n",
    "    if not os.path.exists(FLAGS.log_dir):\n",
    "        os.makedirs(FLAGS.log_dir)\n",
    "    file_handler = logging.FileHandler(pjoin(FLAGS.log_dir, \"log.txt\"))\n",
    "    logging.getLogger().addHandler(file_handler)\n",
    "    \n",
    "    with open(vocab_path, encoding = 'utf8') as file1:\n",
    "        f = file1.readlines()\n",
    "        period_location = [i for i in range(len(f)) if f[i] == '.\\n']\n",
    "\n",
    "    print((vars(FLAGS)))\n",
    "\n",
    "    with open(\"data/squad\"+\"/train.ids.question\", encoding = 'utf8') as t_i_q, open(\"data/squad\" + \"/train.ids.context\", encoding = 'utf8') as t_i_c,\\\n",
    "         open(\"data/squad\" + \"/train.span\", encoding = 'utf8') as t_s, open(\"data/squad\" + \"/val.ids.question\", encoding = 'utf8') as v_i_q,\\\n",
    "         open(\"data/squad\" + \"/val.ids.context\", encoding = 'utf8') as v_i_c, open(\"data/squad\" + \"/val.span\", encoding = 'utf8') as v_s:\n",
    "                q = t_i_q.readlines()\n",
    "                c = t_i_c.readlines()\n",
    "                a = t_s.readlines()\n",
    "                vq = v_i_q.readlines()\n",
    "                vc = v_i_c.readlines()\n",
    "                va = v_s.readlines()\n",
    "    dataset = [[q,c,a],[vq,vc,va]]\n",
    "                \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        load_train_dir = get_normalized_train_dir(FLAGS.load_train_dir or FLAGS.train_dir)\n",
    "        initialize_model(sess, qa, load_train_dir)\n",
    "\n",
    "        save_train_dir = get_normalized_train_dir(FLAGS.train_dir)\n",
    "        qa.train(sess, dataset, FLAGS.epochs, period_location, save_train_dir)\n",
    "#         qa.evaluate_answer(sess, dataset_val, FLAGS.evaluation_size, log=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "#     tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
